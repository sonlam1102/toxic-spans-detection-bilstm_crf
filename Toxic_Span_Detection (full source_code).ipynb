{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Toxic_Span_Detection.ipynb","provenance":[],"collapsed_sections":["dschCt4G8bfB","mFgjWYTTA6Z0","IkRyCbDQA85Y","XsEbmTJsGBca","G2WZCOctatQs","dq3YWgcPFkB1","BmJ-NOODFb0Y","b4XdeIpnE7bC","onI2IhquAwPi","UIgpo4AyFAxO"],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1R-YJ3AXhSSJbaeV6IU1GsuWjRnpY4yd3","authorship_tag":"ABX9TyPDucvmfNmpsDxfxQUW5GWm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dschCt4G8bfB"},"source":["# Install libraries"]},{"cell_type":"markdown","metadata":{"id":"mFgjWYTTA6Z0"},"source":["## BiLSTM-CRF"]},{"cell_type":"code","metadata":{"id":"Nq9Eehn8mZ01","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613028816710,"user_tz":-420,"elapsed":53280,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"0d4f420f-ede1-4c2d-a43a-968be612e584"},"source":[" pip install tensorflow-gpu==1.15.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-gpu==1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411.5MB 37kB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.32.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.12.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.19.5)\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512kB 58.0MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.12.4)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 49.9MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.36.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.10.0)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15.0) (53.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.3.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.7.4.3)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=c301f6c03b6d33c91d4f96ff877d4b156d19fbd998ff0b031763168e8718b76f\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorflow-estimator, gast, tensorboard, keras-applications, tensorflow-gpu\n","  Found existing installation: tensorflow-estimator 2.4.0\n","    Uninstalling tensorflow-estimator-2.4.0:\n","      Successfully uninstalled tensorflow-estimator-2.4.0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorboard 2.4.1\n","    Uninstalling tensorboard-2.4.1:\n","      Successfully uninstalled tensorboard-2.4.1\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swOtuHtAmYkE","executionInfo":{"status":"ok","timestamp":1613028860742,"user_tz":-420,"elapsed":3353,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"9728dbc7-f1e9-4817-a0df-654b36f471f6"},"source":["pip install keras==2.2.4"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.19.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.2)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OfQpKTQNnMJt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613028865951,"user_tz":-420,"elapsed":6879,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"cb7402ae-f64d-4be1-fd3e-409f290a6ff2"},"source":["pip install git+https://www.github.com/keras-team/keras-contrib.git"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-jo0_s2j2\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-jo0_s2j2\n","Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.6/dist-packages\n","Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n","Building wheels for collected packages: keras-contrib\n","  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101066 sha256=1bd853032e6debc4be3f492cf9e59671cd1cc1080e45912cd5940ab5203d0620\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-q9ru71q6/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n","Successfully built keras-contrib\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IkRyCbDQA85Y"},"source":["## Toxic Bert"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"AD8Q7xVmA-pw","executionInfo":{"status":"ok","timestamp":1613030042495,"user_tz":-420,"elapsed":51019,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"c7e167dc-a82c-4ea7-8233-38563be9c018"},"source":["pip install tensorflow-gpu==2.4.0 "],"execution_count":40,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-gpu==2.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/aa/ae64be5acaac9055329289e6bfd54c1efa28bfe792f9021cea495fe2b89d/tensorflow_gpu-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394.7MB 40kB/s \n","\u001b[?25hCollecting tensorboard~=2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/21/eebd23060763fedeefb78bc2b286e00fa1d8abda6f70efa2ee08c28af0d4/tensorboard-2.4.1-py3-none-any.whl (10.6MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.6MB 63.8MB/s \n","\u001b[?25hRequirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.12)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (0.2.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (0.10.0)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (2.10.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.1.2)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (3.12.4)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (3.3.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.1.0)\n","Collecting gast==0.3.3\n","  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.6.3)\n","Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471kB 55.6MB/s \n","\u001b[?25hRequirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.32.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.12.1)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (3.7.4.3)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.15.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (0.36.2)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.19.5)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.25.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (53.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.3.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.4.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (2.23.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (4.7)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.4.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (2.10)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.4.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.1.0)\n","Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow-gpu\n","  Found existing installation: tensorboard 1.15.0\n","    Uninstalling tensorboard-1.15.0:\n","      Successfully uninstalled tensorboard-1.15.0\n","  Found existing installation: gast 0.2.2\n","    Uninstalling gast-0.2.2:\n","      Successfully uninstalled gast-0.2.2\n","  Found existing installation: tensorflow-estimator 1.15.1\n","    Uninstalling tensorflow-estimator-1.15.1:\n","      Successfully uninstalled tensorflow-estimator-1.15.1\n","  Found existing installation: tensorflow-gpu 1.15.0\n","    Uninstalling tensorflow-gpu-1.15.0:\n","      Successfully uninstalled tensorflow-gpu-1.15.0\n","Successfully installed gast-0.3.3 tensorboard-2.4.1 tensorflow-estimator-2.4.0 tensorflow-gpu-2.4.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","tensorboard","tensorflow","tensorflow_estimator"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8TS_KwVA_IF","executionInfo":{"status":"ok","timestamp":1613030049440,"user_tz":-420,"elapsed":57961,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"550cc6b3-0264-4696-9467-7b7189a2031b"},"source":["pip install transformers"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8MB 12.5MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 58.0MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 47.4MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=2fb02f21bcbb7c30bbb0721ece3e5f9f743a23a2b76b44a44bbd1948a782b282\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XsEbmTJsGBca"},"source":["# Data loader"]},{"cell_type":"code","metadata":{"id":"3sRXbs_PWDAx","executionInfo":{"status":"ok","timestamp":1613034428835,"user_tz":-420,"elapsed":1113,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["# Maximum length of comment\n","max_len = 128 \n","# Dimension of embedding vector\n","embedding_dim = 25 \n","# Max feature\n","max_feature = 10000"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Lcz8BrSRC0L","executionInfo":{"status":"ok","timestamp":1613034429221,"user_tz":-420,"elapsed":1489,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["# Read data\n","import pandas as pd\n","from ast import literal_eval\n","\n","data = pd.read_csv('drive/My Drive/CODE/SemVal/dataset/tsd_train.csv')\n","dev = pd.read_csv('drive/My Drive/CODE/SemVal/dataset/tsd_trial.csv')\n","test = pd.read_csv('drive/My Drive/CODE/SemVal/dataset/tsd_test_full.csv')\n","\n","text_data = data['text'].values\n","spans = data['spans'].apply(literal_eval)\n","lbl = [1 if len(s) > 0 else 0 for s in spans]\n","\n","text_data_test = test['text'].values\n","spans_test = test['spans'].apply(literal_eval)\n","test_id = test.index\n","lbl_test = [1 if len(s) > 0 else 0 for s in spans_test]\n","\n","text_data_dev = dev['text'].values\n","spans_dev = dev['spans'].apply(literal_eval)\n","dev_id = dev.index\n","lbl_dev = [1 if len(s) > 0 else 0 for s in spans_dev]"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"8O6SVdILRyAh","executionInfo":{"status":"ok","timestamp":1613034429619,"user_tz":-420,"elapsed":1879,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["# Token level \n","\n","from nltk.tokenize import TweetTokenizer\n","import numpy as np\n","import spacy\n","\n","tknzr2 = TweetTokenizer()\n","\n","def custom_tokenizer(text_data):\n","    return tknzr2.tokenize(text_data)\n","\n","def retrieve_word_from_span(lst_span, text):\n","    i = 0\n","    token = []\n","    a = 0\n","\n","    word = []\n","\n","    while (i < (len(lst_span) - 1)):\n","        if (lst_span[i] != (lst_span[i+1]-1)):\n","            token.append(lst_span[a:(i+1)])\n","            a = i + 1\n","        elif i == (len(lst_span) - 2):\n","            token.append(lst_span[a:i+2])\n","\n","        i = i + 1\n","\n","    for t in token:\n","        word.append(text[t[0]:(t[len(t)-1])+1])\n","\n","    return word\n","\n","def span_retrived(text_data, spans):\n","    token_labels = []\n","\n","    for i in range(0, len(text_data)):\n","        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n","    \n","    return token_labels\n","\n","def span_convert(text_data, spans):\n","    MAX_LEN = 0\n","    token_labels = []\n","\n","    for i in range(0, len(text_data)):\n","        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n","\n","    lst_seq = []\n","    for i in range(0, len(text_data)):\n","        # token = tknzr.tokenize(text_data[i])\n","        token = custom_tokenizer(text_data[i])\n","        if len(token) > MAX_LEN:\n","            MAX_LEN = len(token)\n","            \n","        seq = np.zeros(len(token), dtype=int)\n","        for j in range(0, len(token)):\n","            for t in token_labels[i]:\n","                # if token[j] in tknzr.tokenize(t):\n","                if token[j] in custom_tokenizer(t):\n","                    seq[j] = 1\n","        lst_seq.append(seq)     \n","\n","    return (token_labels, lst_seq)"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"jI5OfwZYPaMO","executionInfo":{"status":"ok","timestamp":1613034436776,"user_tz":-420,"elapsed":9028,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["from copy import deepcopy\n","\n","# convert data\n","data['token'], data['seq'] = span_convert(text_data, spans)\n","dev['token'], dev['seq'] = span_convert(text_data_dev, spans_dev)\n","test['token'], test['seq'] = span_convert(text_data_test, spans_test)\n","\n","train = deepcopy(data)\n","data = pd.concat([data, dev])"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WaHOdIeNKBqd"},"source":["# Evaluation metric "]},{"cell_type":"code","metadata":{"id":"as0mrBcck7eR","executionInfo":{"status":"ok","timestamp":1613031435229,"user_tz":-420,"elapsed":982,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["# Evaluation metric\r\n","\r\n","import sys\r\n","import os\r\n","import os.path\r\n","from scipy.stats import sem\r\n","import numpy as np\r\n","from ast import literal_eval\r\n","\r\n","def f1(predictions, gold):\r\n","    \"\"\"\r\n","    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\r\n","    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\r\n","    :param predictions: a list of predicted offsets\r\n","    :param gold: a list of offsets serving as the ground truth\r\n","    :return: a score between 0 and 1\r\n","    \"\"\"\r\n","    if len(gold) == 0:\r\n","        return 1. if len(predictions) == 0 else 0.\r\n","    if len(predictions) == 0:\r\n","        return 0.\r\n","    predictions_set = set(predictions)\r\n","    gold_set = set(gold)\r\n","    nom = 2 * len(predictions_set.intersection(gold_set))\r\n","    denom = len(predictions_set) + len(gold_set)\r\n","    return float(nom)/float(denom)\r\n","\r\n","\r\n","def evaluate(pred, gold):\r\n","    \"\"\"\r\n","    Based on https://github.com/felipebravom/EmoInt/blob/master/codalab/scoring_program/evaluation.py\r\n","    :param pred: file with predictions\r\n","    :param gold: file with ground truth\r\n","    :return:\r\n","    \"\"\"\r\n","    # # read the predictions\r\n","    # pred_lines = pred.readlines()\r\n","    # # read the ground truth\r\n","    # gold_lines = gold.readlines()\r\n","\r\n","    pred_lines = pred\r\n","    gold_lines = gold\r\n","\r\n","    # only when the same number of lines exists\r\n","    if (len(pred_lines) == len(gold_lines)):\r\n","        data_dic = {}\r\n","        for n, line in enumerate(gold_lines):\r\n","            parts = line.split('\\t')\r\n","            if len(parts) == 2:\r\n","                data_dic[int(parts[0])] = [literal_eval(parts[1])]\r\n","            else:\r\n","                raise ValueError('Format problem for gold line %d.', n)\r\n","\r\n","        for n, line in enumerate(pred_lines):\r\n","            parts = line.split('\\t')\r\n","            if len(parts) == 2:\r\n","                if int(parts[0]) in data_dic:\r\n","                    try:\r\n","                        data_dic[int(parts[0])].append(literal_eval(parts[1]))\r\n","                    except ValueError:\r\n","                        # Invalid predictions are replaced by a default value\r\n","                        data_dic[int(parts[0])].append([])\r\n","                else:\r\n","                    raise ValueError('Invalid text id for pred line %d.', n)\r\n","            else:\r\n","                raise ValueError('Format problem for pred line %d.', n)\r\n","\r\n","        # lists storing gold and prediction scores\r\n","        scores = []\r\n","        for id in data_dic:\r\n","            if len(data_dic[id]) == 2:\r\n","                gold_spans = data_dic[id][0]\r\n","                pred_spans = data_dic[id][1]\r\n","                scores.append(f1(pred_spans, gold_spans))\r\n","            else:\r\n","                sys.exit('Repeated id in test data.')\r\n","\r\n","        return (np.mean(scores), sem(scores))"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G2WZCOctatQs"},"source":["# Data analysis "]},{"cell_type":"code","metadata":{"id":"05MJABVLa97Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613034436786,"user_tz":-420,"elapsed":5537,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"00eeac09-fc55-445d-873e-944768e60fe7"},"source":["# Number of training data\n","print(len(train))"],"execution_count":44,"outputs":[{"output_type":"stream","text":["7939\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3jc4SouVmH4","executionInfo":{"status":"ok","timestamp":1613034436789,"user_tz":-420,"elapsed":4666,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"89f2a98d-7759-4e67-a559-852da86467cf"},"source":["# Number of test data\n","print(len(test))"],"execution_count":45,"outputs":[{"output_type":"stream","text":["2000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"id":"M9gTn4FLV4rL","executionInfo":{"status":"ok","timestamp":1613034436789,"user_tz":-420,"elapsed":2930,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"66317e96-77ee-4ea5-91b6-9f4e522ec65e"},"source":["test"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>spans</th>\n","      <th>text</th>\n","      <th>token</th>\n","      <th>seq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...</td>\n","      <td>That's right. They are not normal. And I am st...</td>\n","      <td>[ABNORMAL, sexist rubbish]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[81, 82, 83, 84, 85, 86]</td>\n","      <td>\"Watch people die from taking away their healt...</td>\n","      <td>[stupid]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[]</td>\n","      <td>tens years ago i contacted the PDR and suggest...</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[]</td>\n","      <td>The parallels between the ANC and the Sicilian...</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[]</td>\n","      <td>Intel Community: â€˜How can we work for a Presid...</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>[4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...</td>\n","      <td>hey loser change your name to something more a...</td>\n","      <td>[loser, ignorant, loser]</td>\n","      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>[23, 24, 25, 26, 27]</td>\n","      <td>And you are a complete moron who obviously doe...</td>\n","      <td>[moron]</td>\n","      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>[157, 158, 159, 160, 161, 162, 163, 164, 165, ...</td>\n","      <td>Such vitriol from the left.  Who would have th...</td>\n","      <td>[hypocrites]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>[]</td>\n","      <td>It is now time for most of you to expand your ...</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>[828, 829, 830, 831]</td>\n","      <td>Why does this author think she can demand, or ...</td>\n","      <td>[dumb]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2000 rows Ã— 4 columns</p>\n","</div>"],"text/plain":["                                                  spans  ...                                                seq\n","0     [84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1                              [81, 82, 83, 84, 85, 86]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","2                                                    []  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","3                                                    []  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","4                                                    []  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","...                                                 ...  ...                                                ...\n","1995  [4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...  ...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n","1996                               [23, 24, 25, 26, 27]  ...  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1997  [157, 158, 159, 160, 161, 162, 163, 164, 165, ...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1998                                                 []  ...         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","1999                               [828, 829, 830, 831]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","\n","[2000 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"tu3EjmPXbB8A","executionInfo":{"status":"ok","timestamp":1613034447824,"user_tz":-420,"elapsed":1043,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"5d4890e3-fb1f-4c29-8dc4-08f32dc16cc3"},"source":["# Show example of training data\n","train.tail(20)"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>spans</th>\n","      <th>text</th>\n","      <th>token</th>\n","      <th>seq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7919</th>\n","      <td>[49, 50, 51, 52, 53, 54]</td>\n","      <td>Ah, so sad. It certainly does seem to bother t...</td>\n","      <td>[trolls]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7920</th>\n","      <td>[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...</td>\n","      <td>Yes, let's end the insanity!  No more trans pe...</td>\n","      <td>[No more trans people! No more boys kissing bo...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7921</th>\n","      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n","      <td>Such garbage logic by republicans which will b...</td>\n","      <td>[Such garbage logic by republicans]</td>\n","      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7922</th>\n","      <td>[33, 34, 35, 36, 37, 38, 39, 40, 41, 96, 97, 9...</td>\n","      <td>aa the usual left wing attack of stupidity is ...</td>\n","      <td>[stupidity, crying that wont stop]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7923</th>\n","      <td>[37, 38, 39, 40, 41]</td>\n","      <td>The G&amp;M doesn't need to repeat EVERY idiot thi...</td>\n","      <td>[idiot]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7924</th>\n","      <td>[177, 178, 179, 180, 181, 182, 183, 184, 185]</td>\n","      <td>All this reality tv suspense, all this loud no...</td>\n","      <td>[stupidity]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7925</th>\n","      <td>[15, 16, 17, 18, 19, 20]</td>\n","      <td>Can't get over idiots like yourself who probab...</td>\n","      <td>[idiots]</td>\n","      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7926</th>\n","      <td>[31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 46, 4...</td>\n","      <td>I only use the word haole when stupidity and a...</td>\n","      <td>[stupidity, arrogance]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7927</th>\n","      <td>[132, 133, 134, 135]</td>\n","      <td>Pity Vegter doesn't apply the same rigour he n...</td>\n","      <td>[dick]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7928</th>\n","      <td>[54, 55, 56, 57]</td>\n","      <td>Anyone who voted for Hillary is blind, gullibl...</td>\n","      <td>[fool]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7929</th>\n","      <td>[26, 27, 28, 29, 30, 114, 115, 116, 117, 118, ...</td>\n","      <td>No accountability on both sides. No clue on wh...</td>\n","      <td>[sides, idiots]</td>\n","      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7930</th>\n","      <td>[320, 321, 322, 323, 324, 350, 351, 352, 389, ...</td>\n","      <td>Yes, it is OK. I haven't seen any rioting and ...</td>\n","      <td>[dirty, gay, racist, morons]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7931</th>\n","      <td>[44, 45, 46, 47, 48, 49, 50, 51]</td>\n","      <td>In other word, blah, blah, blah, blah. It's bu...</td>\n","      <td>[bullshit]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7932</th>\n","      <td>[218, 219, 220, 221, 222, 223]</td>\n","      <td>Driving like that is why people are going to s...</td>\n","      <td>[idiots]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7933</th>\n","      <td>[4, 5, 6, 7, 8, 35, 36, 37, 38]</td>\n","      <td>The idiot in chief can't speak for sh!t ðŸ˜œðŸ‘Ž</td>\n","      <td>[idiot, sh!t]</td>\n","      <td>[0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7934</th>\n","      <td>[8, 9, 10, 11]</td>\n","      <td>Another fool pipes in.</td>\n","      <td>[fool]</td>\n","      <td>[0, 1, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7935</th>\n","      <td>[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...</td>\n","      <td>So if a restaurant owner puts up a sign saying...</td>\n","      <td>[No Blacks Allowed]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7936</th>\n","      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n","      <td>Any faith that can't stand up to logic and rea...</td>\n","      <td>[Any faith that can't stand up to logic and re...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7937</th>\n","      <td>[5, 6, 7, 8, 9, 10, 11]</td>\n","      <td>This idiotic. Use the surplus to pay down the ...</td>\n","      <td>[idiotic]</td>\n","      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7938</th>\n","      <td>[106, 107, 108, 109, 110, 169, 170, 171, 172, ...</td>\n","      <td>Who is this \"we\" of which you speak? Are you r...</td>\n","      <td>[penis, women's bodies]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  spans  ...                                                seq\n","7919                           [49, 50, 51, 52, 53, 54]  ...         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n","7920  [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...  ...  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","7921  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  ...   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","7922  [33, 34, 35, 36, 37, 38, 39, 40, 41, 96, 97, 9...  ...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n","7923                               [37, 38, 39, 40, 41]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n","7924      [177, 178, 179, 180, 181, 182, 183, 184, 185]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7925                           [15, 16, 17, 18, 19, 20]  ...  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7926  [31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 46, 4...  ...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ...\n","7927                               [132, 133, 134, 135]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7928                                   [54, 55, 56, 57]  ...            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n","7929  [26, 27, 28, 29, 30, 114, 115, 116, 117, 118, ...  ...  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7930  [320, 321, 322, 323, 324, 350, 351, 352, 389, ...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7931                   [44, 45, 46, 47, 48, 49, 50, 51]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n","7932                     [218, 219, 220, 221, 222, 223]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7933                    [4, 5, 6, 7, 8, 35, 36, 37, 38]  ...               [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n","7934                                     [8, 9, 10, 11]  ...                                    [0, 1, 0, 0, 0]\n","7935  [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, ...\n","7936  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  ...         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n","7937                            [5, 6, 7, 8, 9, 10, 11]  ...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7938  [106, 107, 108, 109, 110, 169, 170, 171, 172, ...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","\n","[20 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"viHoxDqEauyj","executionInfo":{"status":"ok","timestamp":1613034451343,"user_tz":-420,"elapsed":1144,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["# counting word in spans for train \n","len_span = train['token'].apply(len)"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"sw7qNdR-VppI","executionInfo":{"status":"ok","timestamp":1613034452157,"user_tz":-420,"elapsed":1146,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["# counting word in spans for test \n","len_span_test = test['token'].apply(len)"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrGstC0whNKs","executionInfo":{"status":"ok","timestamp":1613034465286,"user_tz":-420,"elapsed":1077,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"84f32782-36eb-4a21-ef8a-a26a080f8d55"},"source":["# Statistic spans by number of word in span for train \n","len_span.value_counts(normalize=True, sort=True)"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1     0.676534\n","2     0.200655\n","0     0.061091\n","3     0.043330\n","4     0.011588\n","5     0.003401\n","6     0.001512\n","7     0.001008\n","8     0.000504\n","11    0.000126\n","25    0.000126\n","9     0.000126\n","Name: token, dtype: float64"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUR6XwF2VuEz","executionInfo":{"status":"ok","timestamp":1613034459712,"user_tz":-420,"elapsed":1008,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"4a01a550-5700-4d4e-bca8-b91d7d65dea5"},"source":["# Statistic spans by number of word in span for test \n","len_span_test.value_counts(normalize=True, sort=True)"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    0.7035\n","0    0.1970\n","2    0.0860\n","3    0.0080\n","4    0.0040\n","6    0.0010\n","7    0.0005\n","Name: token, dtype: float64"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"gPJWuIpegKo1","executionInfo":{"status":"ok","timestamp":1613034461050,"user_tz":-420,"elapsed":1446,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"6d3b9b0f-1523-42bc-eabc-09d6bf1d4c62"},"source":["# Distribution histogram plot\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","%matplotlib inline\n","\n","fig, ax = plt.subplots()\n","\n","ax.hist(len_span, density=True, edgecolor='k', rwidth=0.8)  # density=False would make counts\n","\n","plt.ylabel('Percentage')\n","plt.xlabel('Number of span');"],"execution_count":51,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"dq3YWgcPFkB1"},"source":["# Word embedding"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s4txBO5zya-5","executionInfo":{"status":"ok","timestamp":1613030249939,"user_tz":-420,"elapsed":19480,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"a3703690-324f-4d99-c80a-0b4171c205b0"},"source":["# Read embedding\n","word_dict = []\n","embeddings_index = {}\n","f = open('drive/My Drive/CODE/SemVal/embedding/glove.twitter.27B.25d.txt')\n","for line in f:\n","    values = line.split(' ')\n","    word = values[0] \n","    word_dict.append(word)\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('GloVe data loaded')\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["GloVe data loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n8bsiH8BSGjC","executionInfo":{"status":"ok","timestamp":1613030250488,"user_tz":-420,"elapsed":20020,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["words = word_dict\n","num_words = len(words)\n","\n","# Dictionary word:index pair\n","# word is key and its value is corresponding index\n","word_to_index = {w : i + 2 for i, w in enumerate(words)}\n","word_to_index[\"UNK\"] = 1\n","word_to_index[\"PAD\"] = 0\n","\n","# Dictionary lable:index pair\n","idx2word = {i: w for w, i in word_to_index.items()}"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OBCNzNnas7p","executionInfo":{"status":"ok","timestamp":1613030250489,"user_tz":-420,"elapsed":20016,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["# first create a matrix of zeros, this is our embedding matrix\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","\n","# for each word in out tokenizer lets try to find that work in our w2v model\n","for word, i in word_to_index.items():\n","    if i > max_feature:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # we found the word - add that words vector to the matrix\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # doesn't exist, assign a random vector\n","        embedding_matrix[i] = np.random.randn(embedding_dim)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"exfoi9XHba3G","executionInfo":{"status":"ok","timestamp":1613030250489,"user_tz":-420,"elapsed":20008,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":[" # mapping for token cases\r\n","case2Idx = {'1': 1, '0': 0}\r\n","caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \r\n","\r\n","char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\r\n","for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\r\n","    char2Idx[c] = len(char2Idx)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BmJ-NOODFb0Y"},"source":["# Data pre-processing "]},{"cell_type":"code","metadata":{"id":"ZpsRfA9QF9mL","executionInfo":{"status":"ok","timestamp":1613030250491,"user_tz":-420,"elapsed":18736,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","y = data['seq']\n","X = data['text']\n","\n","y_test = test['seq']\n","X_test = test['text']"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"bsS9aVEVcJQO","executionInfo":{"status":"ok","timestamp":1613030250491,"user_tz":-420,"elapsed":18731,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["#train test\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size = 0.1)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z8GGEAurVu_c","executionInfo":{"status":"ok","timestamp":1613030250911,"user_tz":-420,"elapsed":19143,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"17065691-6cc6-496a-d90e-d8b860869a20"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","from nltk.tokenize import TweetTokenizer\n","from nltk.tokenize import word_tokenize\n","from nltk import WordNetLemmatizer\n","\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.initializers import Constant\n","from nltk.corpus import stopwords\n","import re\n","import numpy as np\n","\n","tknzr2 = TweetTokenizer()\n","\n","def custom_tokenizer(text_data):\n","    text_data = text_data.lower()\n","    return tknzr2.tokenize(text_data)\n","\n","def preprocess(text):\n","    text = text.lower()\n","\n","    word_list = nltk.word_tokenize(text)\n","    lemma = WordNetLemmatizer()\n","\n","    for w in word_list:\n","        w = lemma.lemmatize(w)\n","\n","    new_text = \"\"\n","    for w in word_list:\n","        new_text = new_text + \" \" + w\n","\n","    return new_text\n","\n","def encoding(X, y, isTest = True):\n","    sentences = []\n","    \n","    for t in X:\n","        sentences.append(custom_tokenizer(t))\n","\n","    X = []\n","    for s in sentences:\n","        sent = []\n","        for w in s:\n","            try:\n","                w = w.lower()\n","                sent.append(word_to_index[w])\n","            except:\n","                sent.append(word_to_index[\"UNK\"])\n","        X.append(sent)\n","           \n","    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])\n","\n","    if isTest:\n","        y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=word_to_index[\"PAD\"])\n","        y = to_categorical(y, num_classes=2)\n","    else:\n","        y = None\n","\n","    return (X,y)\n","\n","\n","def decoding(text_data, encoding_text, prediction):\n","    test = [[idx2word[i] for i in row] for row in encoding_text]\n","\n","    lst_token = []\n","\n","    for t in range(0, len(test)):\n","        yy_pred = []\n","        for i in range(0, len(test[t])):\n","            if prediction[t][i] == 1:\n","                yy_pred.append(test[t][i])\n","        lst_token.append(yy_pred)\n","\n","    lis_idx = []\n","    for i in range(0, len(text_data)):\n","        idx = []\n","        for t in lst_token[i]:\n","            index = text_data[i].find(t)\n","            idx.append(index)\n","            for j in range(1, len(t)):\n","                index = index + 1\n","                idx.append(index)\n","        lis_idx.append(idx)\n","\n","    return lis_idx"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"9ajE_t3U-PiF","executionInfo":{"status":"ok","timestamp":1613030252615,"user_tz":-420,"elapsed":20842,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["X1, y1 = encoding(X_train, y_train)\r\n","X2, y2 = encoding(X_dev, y_dev)\r\n","X3, y3 = encoding(X_test, y_test)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozUGBMYyIAAu","executionInfo":{"status":"ok","timestamp":1613030253088,"user_tz":-420,"elapsed":21310,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"7912ee18-23c8-469a-c207-492183eb67c8"},"source":["# Illustrating the data transforming \n","x_t, y_t = encoding(X, y)\n","\n","print(custom_tokenizer(X[7926]))\n","print(X[7926])\n","print(x_t[7926])\n","\n","print(y[7926])"],"execution_count":18,"outputs":[{"output_type":"stream","text":["['i', 'only', 'use', 'the', 'word', 'haole', 'when', 'stupidity', 'and', 'arrogance', 'is', 'involved', 'and', 'not', 'all', 'the', 'time', '.', 'excluding', 'the', 'potus', 'of', 'course', '.']\n","I only use the word haole when stupidity and arrogance is involved and not all the time.  Excluding the POTUS of course.\n","[    12    216    718     15    894 724236     94  17046     28  48680\n","     34   6187     28     80     77     15    137      3  81507     15\n","  34058     41   1605      3      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0]\n","[0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b4XdeIpnE7bC"},"source":["# Detection"]},{"cell_type":"code","metadata":{"id":"h_BbOL6QMW2_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613017772467,"user_tz":-420,"elapsed":9931,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"f79f69ff-1169-4ad8-cf9d-86b16fe75500"},"source":["# BiLSTM - CRF \n","from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional, Flatten, Dropout\n","from keras.models import Model, Input\n","from keras_contrib.layers import CRF\n","from keras.utils import plot_model\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# from keras.metrics import BinaryAccuracy, Precision, Recall, AUC\n","\n","input = Input(shape = (max_len,))\n","model = Embedding(input_dim=num_words+2,\n","                    output_dim=embedding_dim,\n","                    embeddings_initializer=Constant(embedding_matrix),\n","                    input_length=max_len,\n","                    trainable=True)(input)\n","\n","model = Dropout(0.1)(model)\n","model = Bidirectional(LSTM(units = max_len, return_sequences=True, recurrent_dropout=0.1))(model)\n","model = TimeDistributed(Dense(max_len, activation=\"relu\"))(model)\n","crf = CRF(2)  \n","out = crf(model)  # output\n","\n","model = Model(input, out)\n","model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=['accuracy'])\n","\n","model.summary()\n","\n","plot_model(model,to_file=\"bilstm-crf.pdf\",show_shapes=True,show_layer_names=True)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_3 (InputLayer)         (None, 128)               0         \n","_________________________________________________________________\n","embedding_3 (Embedding)      (None, 128, 25)           29837900  \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 128, 25)           0         \n","_________________________________________________________________\n","bidirectional_3 (Bidirection (None, 128, 256)          157696    \n","_________________________________________________________________\n","time_distributed_3 (TimeDist (None, 128, 128)          32896     \n","_________________________________________________________________\n","crf_3 (CRF)                  (None, 128, 2)            266       \n","=================================================================\n","Total params: 30,028,758\n","Trainable params: 30,028,758\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YnDO9T5td0wu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611720774064,"user_tz":-420,"elapsed":954503,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"5ae495c6-8ec1-4fc7-fd45-b984f822662c"},"source":["from keras.callbacks import ModelCheckpoint\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","checkpointer = ModelCheckpoint(filepath = 'drive/My Drive/CODE/SemVal/model/model_detection_19.h5',\n","                       verbose = 0,\n","                       mode = 'auto',\n","                       save_best_only = True,\n","                       monitor='val_loss')\n","\n","model.fit(X1, np.array(y1), batch_size=64, epochs=15, validation_data=(X2, y2), callbacks=[checkpointer])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Train on 7766 samples, validate on 863 samples\n","Epoch 1/15\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","7766/7766 [==============================] - 66s 8ms/step - loss: 0.1088 - acc: 0.9736 - val_loss: 0.0888 - val_acc: 0.9778\n","Epoch 2/15\n","7766/7766 [==============================] - 60s 8ms/step - loss: 0.0937 - acc: 0.9736 - val_loss: 0.0825 - val_acc: 0.9790\n","Epoch 3/15\n","7766/7766 [==============================] - 59s 8ms/step - loss: 0.0864 - acc: 0.9736 - val_loss: 0.0783 - val_acc: 0.9797\n","Epoch 4/15\n","7766/7766 [==============================] - 58s 8ms/step - loss: 0.0812 - acc: 0.9736 - val_loss: 0.0746 - val_acc: 0.9797\n","Epoch 5/15\n","7766/7766 [==============================] - 60s 8ms/step - loss: 0.0761 - acc: 0.9736 - val_loss: 0.0734 - val_acc: 0.9794\n","Epoch 6/15\n","7766/7766 [==============================] - 59s 8ms/step - loss: 0.0718 - acc: 0.9736 - val_loss: 0.0698 - val_acc: 0.9798\n","Epoch 7/15\n","7766/7766 [==============================] - 59s 8ms/step - loss: 0.0666 - acc: 0.9736 - val_loss: 0.0690 - val_acc: 0.9798\n","Epoch 8/15\n","7766/7766 [==============================] - 58s 7ms/step - loss: 0.0600 - acc: 0.9736 - val_loss: 0.0708 - val_acc: 0.9789\n","Epoch 9/15\n","7766/7766 [==============================] - 57s 7ms/step - loss: 0.0522 - acc: 0.9736 - val_loss: 0.0699 - val_acc: 0.9783\n","Epoch 10/15\n","7766/7766 [==============================] - 57s 7ms/step - loss: 0.0485 - acc: 0.9736 - val_loss: 0.0720 - val_acc: 0.9777\n","Epoch 11/15\n","7766/7766 [==============================] - 57s 7ms/step - loss: 0.0459 - acc: 0.9736 - val_loss: 0.0717 - val_acc: 0.9776\n","Epoch 12/15\n","7766/7766 [==============================] - 57s 7ms/step - loss: 0.0436 - acc: 0.9736 - val_loss: 0.0733 - val_acc: 0.9787\n","Epoch 13/15\n","7766/7766 [==============================] - 57s 7ms/step - loss: 0.0417 - acc: 0.9736 - val_loss: 0.0709 - val_acc: 0.9768\n","Epoch 14/15\n","7766/7766 [==============================] - 57s 7ms/step - loss: 0.0402 - acc: 0.9736 - val_loss: 0.0752 - val_acc: 0.9748\n","Epoch 15/15\n","7766/7766 [==============================] - 57s 7ms/step - loss: 0.0384 - acc: 0.9736 - val_loss: 0.0738 - val_acc: 0.9772\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fba0915d9e8>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"onI2IhquAwPi"},"source":["# Classification "]},{"cell_type":"code","metadata":{"id":"N0wNXoS4Ax1J"},"source":["import numpy as np \n","import pandas as pd \n","from transformers import DistilBertTokenizerFast\n","tokenizer = DistilBertTokenizerFast.from_pretrained('unitary/toxic-bert')\n","\n","import torch\n","\n","class BuildDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","X_train_classify = text_data\n","y_train_classify = lbl\n","\n","X_test_classify = text_data_test\n","y_test_classify = lbl_test\n","\n","\n","train_encodings = tokenizer(X_train_classify.tolist(), truncation=True, padding=True, max_length=max_len)\n","test_encodings = tokenizer(X_test_classify.tolist(), truncation=True, padding=True, max_length=max_len)\n","\n","# train = pd.DataFrame({'text': train_encodings, 'labels': y_train_classify})\n","# # dev = pd.DataFrame({'text': dev_X, 'labels': dev_y})\n","# test = pd.DataFrame({'text': test_encodings, 'labels': y_test_classify})\n","\n","# train = pd.concat([train, test])\n","\n","train_dataset = BuildDataset(train_encodings, y_train_classify)\n","test_dataset = BuildDataset(test_encodings, y_test_classify)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hdUD-pNU6ua"},"source":["from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, AutoModelForSequenceClassification\n","\n","\n","training_args = TrainingArguments(\n","    output_dir='drive/MyDrive/CODE/SemVal/results',          # output directory\n","    num_train_epochs=3,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n",")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=test_dataset             # evaluation dataset\n",")\n","\n","trainer.train()\n","\n","trainer.save_model('drive/MyDrive/CODE/SemVal/model/transformers2/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yG2B766GfFXX"},"source":["y_pred_classify = trainer.predict(test_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRpJg0QYkCbf"},"source":["from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score\n","\n","y_pred = y_pred_classify.label_ids\n","y_true = y_test_classify\n","\n","cf = confusion_matrix(y_true, y_pred)\n","print(cf)\n","\n","evaluation = f1_score(y_true, y_pred, average='micro')\n","\n","print(\"F1 - micro: \" + str(evaluation))\n","\n","evaluation = f1_score(y_true, y_pred, average='macro')\n","print(\"F1 - macro: \" + str(evaluation))\n","\n","evaluation = recall_score(y_true, y_pred)\n","print(\"Recall: \" + str(evaluation))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UIgpo4AyFAxO"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"Q4M5Pg396yM8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613029739065,"user_tz":-420,"elapsed":96487,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"cdecffc2-4579-4ca3-8f84-515996a71016"},"source":["# Load bi-LSTM model\n","\n","from keras.models import load_model\n","from keras_contrib.layers import CRF\n","from keras_contrib.losses import crf_loss\n","from keras_contrib.metrics import crf_accuracy\n","\n","# load model\n","model = load_model('drive/My Drive/CODE/SemVal/model/model_detection_4.h5', custom_objects={'CRF':CRF,'crf_loss':crf_loss,'crf_accuracy':crf_accuracy})"],"execution_count":33,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0K_jXnF2nVVV","executionInfo":{"status":"ok","timestamp":1613029870445,"user_tz":-420,"elapsed":16442,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["y_pred = model.predict(X3)\n","y_pred = np.argmax(y_pred, axis=-1)\n","y_test_true = np.argmax(y3, -1)\n","# y_pred = [[i for i in row] for row in y_pred]"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RQpdMVR17DL","executionInfo":{"status":"ok","timestamp":1613029870446,"user_tz":-420,"elapsed":14622,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["# return back\n","test = [[idx2word[i] for i in row] for row in X3]"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MyoE9-1e6pbu","executionInfo":{"status":"ok","timestamp":1613029870446,"user_tz":-420,"elapsed":13049,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"13e127a1-a9b7-4767-8329-de2973864894"},"source":["yy_pred = []\n","yy_test = []\n","\n","for i in range(0, len(test[0])):\n","    if y_pred[0][i] == 1:\n","        yy_pred.append(test[0][i])\n","\n","for i in range(0, len(test[0])):\n","    if y_test_true[0][i] == 1:\n","        yy_test.append(test[0][i])\n","\n","print(yy_pred)\n","print(yy_test)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["['sexist', 'rubbish']\n","['abnormal', 'sexist', 'rubbish']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eDUPWplLuZZw","executionInfo":{"status":"ok","timestamp":1613029870857,"user_tz":-420,"elapsed":11085,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["raw_y = decoding(X_test, X3, y_pred)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BKFHGePtTwt","executionInfo":{"status":"ok","timestamp":1613029870859,"user_tz":-420,"elapsed":9974,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"59145e5a-5e67-4052-8ab7-df2f31eac2ec"},"source":["f1(raw_y[0], spans_test[0])\n","\n","acc = []\n","for i in range(0, len(spans_test)):\n","    acc.append(f1(raw_y[i], spans_test[i]))\n","\n","print(np.mean(acc)*100)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["61.320465302853854\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"07LODzVr3Ntk","executionInfo":{"status":"ok","timestamp":1613029920922,"user_tz":-420,"elapsed":1520,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["# Make CSV file for itegrate BERT \r\n","new_sub = pd.DataFrame({'id': test_id, 'text': text_data_test, 'spans': raw_y, 'span_true': spans_test})\r\n","\r\n","new_sub.to_csv('drive/My Drive/CODE/SemVal/test_demo_detection.csv', index=False)"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zXMldDNCpxCv"},"source":["# Evalutation 2"]},{"cell_type":"code","metadata":{"id":"NR-C3l-1TRji","executionInfo":{"status":"ok","timestamp":1613030424158,"user_tz":-420,"elapsed":1078,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["########################################\r\n","########################################\r\n","#######################################\r\n","\r\n","import pandas as pd\r\n","from ast import literal_eval\r\n","\r\n","test = pd.read_csv('drive/My Drive/CODE/SemVal/test_demo_detection.csv')\r\n","\r\n","test_text_data = test['text'].values\r\n","test_spans = test['spans'].apply(literal_eval)\r\n","test_id = test['id']\r\n","test_spans_true = test['span_true'].apply(literal_eval)\r\n","lbl_test = [1 if len(s) > 0 else 0 for s in test_spans_true]"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"5TQ-4PROTrsf","executionInfo":{"status":"ok","timestamp":1613030431122,"user_tz":-420,"elapsed":6989,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["from transformers import DistilBertTokenizerFast, TrainingArguments, AutoModelForSequenceClassification, Trainer\r\n","\r\n","tokenizer = DistilBertTokenizerFast.from_pretrained('unitary/toxic-bert')\r\n","model_classify = AutoModelForSequenceClassification.from_pretrained('drive/MyDrive/CODE/SemVal/model/transformers2', local_files_only=True)\r\n","\r\n","import torch\r\n","class BuildDataset(torch.utils.data.Dataset):\r\n","    def __init__(self, encodings, labels):\r\n","        self.encodings = encodings\r\n","        self.labels = labels\r\n","\r\n","    def __getitem__(self, idx):\r\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n","        item['labels'] = torch.tensor(self.labels[idx])\r\n","        return item\r\n","\r\n","    def __len__(self):\r\n","        return len(self.labels)\r\n","\r\n","test_encodings = tokenizer(test_text_data.tolist(), truncation=True, padding=True)\r\n","test_dataset = BuildDataset(test_encodings, lbl_test)\r\n","\r\n","trainer = Trainer(model=model_classify, eval_dataset = test_dataset)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"bIJsW8bxTxb3","executionInfo":{"status":"ok","timestamp":1613030442762,"user_tz":-420,"elapsed":15492,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"b6f8550e-225d-42c9-f767-d1cb723fe317"},"source":["raw_y_classify = trainer.predict(test_dataset)\r\n","\r\n","new_span_test = []\r\n","\r\n","for i in range(0, len(test_spans)):\r\n","    if raw_y_classify.label_ids[i] == 0:\r\n","        new_span_test.append([])\r\n","    else:\r\n","        new_span_test.append(test_spans[i])"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [250/250 00:11]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TAm4lHmuUBhw","executionInfo":{"status":"ok","timestamp":1613031372383,"user_tz":-420,"elapsed":1141,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"67a6975b-14d6-44d9-b310-6c3a51df72cd"},"source":["acc = []\r\n","for i in range(0, len(test_spans)):\r\n","    acc.append(f1(new_span_test[i], test_spans[i]))\r\n","\r\n","print(np.mean(acc))"],"execution_count":29,"outputs":[{"output_type":"stream","text":["0.854\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P-Xj7HP0RiqW","executionInfo":{"status":"ok","timestamp":1613033306287,"user_tz":-420,"elapsed":1146,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}}},"source":["# Make CSV file for itegrate BERT \n","new_sub = pd.DataFrame({'id': test_id, 'text': text_data_test, 'spans': new_span_test, 'span_true': test_spans_true})\n","\n","new_sub.to_csv('drive/My Drive/CODE/SemVal/test_demo_classification.csv', index=False)"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLzznY_nwpac"},"source":["# Submission"]},{"cell_type":"code","metadata":{"id":"scubUW_tkDHX"},"source":["# Load bi-LSTM model\n","\n","from keras.models import load_model\n","from keras_contrib.layers import CRF\n","from keras_contrib.losses import crf_loss\n","from keras_contrib.metrics import crf_accuracy\n","\n","# load model\n","model = load_model('drive/My Drive/CODE/SemVal/model/model_detection_4.h5', custom_objects={'CRF':CRF,'crf_loss':crf_loss,'crf_accuracy':crf_accuracy})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66brisWy5Y2i"},"source":["SUBMIUSSION = 'drive/My Drive/CODE/SemVal/dataset/tsd_test.csv'\r\n","\r\n","subm = pd.read_csv(SUBMIUSSION)\r\n","subm_text = subm['text']\r\n","subm_id = subm.index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9XO7WRwi7c38"},"source":["X4, y4 = encoding(subm_text, None, isTest=False)\r\n","\r\n","y_sub = model.predict(X4)\r\n","y_sub = np.argmax(y_sub, axis=-1)\r\n","\r\n","raw_subm = decoding(subm_text, X4, y_sub)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJ6RpjxDuP5M"},"source":["# Make CSV file \n","new_sub = pd.DataFrame({'id': subm_id, 'text': subm_text, 'spans': raw_subm})\n","\n","new_sub.to_csv('drive/My Drive/CODE/SemVal/submission_detection.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iv_MbxGOnIQl"},"source":["# make sure that the ids match the ones of the scores\n","predictions = raw_subm\n","ids = subm_id\n","\n","# write in a prediction file named \"spans-pred.txt\"\n","with open(\"spans-pred.txt\", \"w\") as out:\n","  for uid, text_scores in zip(ids, predictions):\n","    out.write(f\"{str(uid)}\\t{str(text_scores)}\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9nq9qGswXw6"},"source":["# Grab them toghether"]},{"cell_type":"code","metadata":{"id":"gd08oo5Mxkuy"},"source":["import pandas as pd\n","\n","subm = pd.read_csv('drive/My Drive/CODE/SemVal/submission_detection.csv')\n","from ast import literal_eval\n","\n","subm_text_data = subm['text'].values\n","subm_spans = subm['spans'].apply(literal_eval)\n","subm_id = subm['id']\n","\n","lbl_subm = [1 if len(s) > 0 else 0 for s in subm_spans]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ng66jOrqwgPA"},"source":["from transformers import DistilBertTokenizerFast, TrainingArguments, AutoModelForSequenceClassification, Trainer\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained('unitary/toxic-bert')\n","model_classify = AutoModelForSequenceClassification.from_pretrained('drive/MyDrive/CODE/SemVal/model/transformers2', local_files_only=True)\n","\n","import torch\n","class BuildDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","subm_encodings = tokenizer(subm_text_data.tolist(), truncation=True, padding=True)\n","subm_dataset = BuildDataset(subm_encodings, lbl_subm)\n","\n","trainer = Trainer(model=model_classify, eval_dataset = subm_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"kCNhxK5M0HkX","executionInfo":{"status":"ok","timestamp":1611587350673,"user_tz":-420,"elapsed":26052,"user":{"displayName":"SÆ¡n LÆ°u Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"bb44956d-286a-4476-a22a-86376aabe3d2"},"source":["raw_y_classify = trainer.predict(subm_dataset)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [250/250 00:23]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"d5AtRq4B02gJ"},"source":["new_span_subm = []\n","\n","for i in range(0, len(subm_spans)):\n","    if raw_y_classify.label_ids[i] == 0:\n","        new_span_subm.append([])\n","    else:\n","        new_span_subm.append(subm_spans[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0eMzw0hq18Ej"},"source":["# make sure that the ids match the ones of the scores\n","predictions = new_span_subm\n","ids = subm_id\n","\n","# write in a prediction file named \"spans-pred.txt\"\n","with open(\"spans-pred.txt\", \"w\") as out:\n","  for uid, text_scores in zip(ids, predictions):\n","    out.write(f\"{str(uid)}\\t{str(text_scores)}\\n\")"],"execution_count":null,"outputs":[]}]}