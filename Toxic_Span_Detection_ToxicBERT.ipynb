{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Toxic_Span_Detection_ToxicBERT.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1R-YJ3AXhSSJbaeV6IU1GsuWjRnpY4yd3","authorship_tag":"ABX9TyP04rFC/qDtO0xNyms7MoeZ"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dschCt4G8bfB"},"source":["# Install libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"AD8Q7xVmA-pw","executionInfo":{"status":"ok","timestamp":1613030042495,"user_tz":-420,"elapsed":51019,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"c7e167dc-a82c-4ea7-8233-38563be9c018"},"source":["pip install tensorflow-gpu==2.4.0 "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-gpu==2.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/aa/ae64be5acaac9055329289e6bfd54c1efa28bfe792f9021cea495fe2b89d/tensorflow_gpu-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7MB)\n","\u001b[K     |████████████████████████████████| 394.7MB 40kB/s \n","\u001b[?25hCollecting tensorboard~=2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/21/eebd23060763fedeefb78bc2b286e00fa1d8abda6f70efa2ee08c28af0d4/tensorboard-2.4.1-py3-none-any.whl (10.6MB)\n","\u001b[K     |████████████████████████████████| 10.6MB 63.8MB/s \n","\u001b[?25hRequirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.12)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (0.2.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (0.10.0)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (2.10.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.1.2)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (3.12.4)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (3.3.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.1.0)\n","Collecting gast==0.3.3\n","  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.6.3)\n","Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\n","\u001b[K     |████████████████████████████████| 471kB 55.6MB/s \n","\u001b[?25hRequirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.32.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.12.1)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (3.7.4.3)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.15.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (0.36.2)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.4.0) (1.19.5)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.25.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (53.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.3.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.4.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (2.23.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (4.7)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.4.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (2.10)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.4.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.1.0)\n","Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow-gpu\n","  Found existing installation: tensorboard 1.15.0\n","    Uninstalling tensorboard-1.15.0:\n","      Successfully uninstalled tensorboard-1.15.0\n","  Found existing installation: gast 0.2.2\n","    Uninstalling gast-0.2.2:\n","      Successfully uninstalled gast-0.2.2\n","  Found existing installation: tensorflow-estimator 1.15.1\n","    Uninstalling tensorflow-estimator-1.15.1:\n","      Successfully uninstalled tensorflow-estimator-1.15.1\n","  Found existing installation: tensorflow-gpu 1.15.0\n","    Uninstalling tensorflow-gpu-1.15.0:\n","      Successfully uninstalled tensorflow-gpu-1.15.0\n","Successfully installed gast-0.3.3 tensorboard-2.4.1 tensorflow-estimator-2.4.0 tensorflow-gpu-2.4.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","tensorboard","tensorflow","tensorflow_estimator"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8TS_KwVA_IF","executionInfo":{"status":"ok","timestamp":1613030049440,"user_tz":-420,"elapsed":57961,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"550cc6b3-0264-4696-9467-7b7189a2031b"},"source":["pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 12.5MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 58.0MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 47.4MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=2fb02f21bcbb7c30bbb0721ece3e5f9f743a23a2b76b44a44bbd1948a782b282\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XsEbmTJsGBca"},"source":["# Data loader"]},{"cell_type":"code","metadata":{"id":"3sRXbs_PWDAx"},"source":["# Maximum length of comment\n","max_len = 128 \n","# Dimension of embedding vector\n","embedding_dim = 25 \n","# Max feature\n","max_feature = 10000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Lcz8BrSRC0L"},"source":["# Read data\n","import pandas as pd\n","from ast import literal_eval\n","\n","data = pd.read_csv('drive/My Drive/CODE/SemVal/dataset/tsd_train.csv')\n","dev = pd.read_csv('drive/My Drive/CODE/SemVal/dataset/tsd_trial.csv')\n","test = pd.read_csv('drive/My Drive/CODE/SemVal/dataset/tsd_test_full.csv')\n","\n","text_data = data['text'].values\n","spans = data['spans'].apply(literal_eval)\n","lbl = [1 if len(s) > 0 else 0 for s in spans]\n","\n","text_data_test = test['text'].values\n","spans_test = test['spans'].apply(literal_eval)\n","test_id = test.index\n","lbl_test = [1 if len(s) > 0 else 0 for s in spans_test]\n","\n","text_data_dev = dev['text'].values\n","spans_dev = dev['spans'].apply(literal_eval)\n","dev_id = dev.index\n","lbl_dev = [1 if len(s) > 0 else 0 for s in spans_dev]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8O6SVdILRyAh"},"source":["# Token level \n","\n","from nltk.tokenize import TweetTokenizer\n","import numpy as np\n","import spacy\n","\n","tknzr2 = TweetTokenizer()\n","\n","def custom_tokenizer(text_data):\n","    return tknzr2.tokenize(text_data)\n","\n","def retrieve_word_from_span(lst_span, text):\n","    i = 0\n","    token = []\n","    a = 0\n","\n","    word = []\n","\n","    while (i < (len(lst_span) - 1)):\n","        if (lst_span[i] != (lst_span[i+1]-1)):\n","            token.append(lst_span[a:(i+1)])\n","            a = i + 1\n","        elif i == (len(lst_span) - 2):\n","            token.append(lst_span[a:i+2])\n","\n","        i = i + 1\n","\n","    for t in token:\n","        word.append(text[t[0]:(t[len(t)-1])+1])\n","\n","    return word\n","\n","def span_retrived(text_data, spans):\n","    token_labels = []\n","\n","    for i in range(0, len(text_data)):\n","        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n","    \n","    return token_labels\n","\n","def span_convert(text_data, spans):\n","    MAX_LEN = 0\n","    token_labels = []\n","\n","    for i in range(0, len(text_data)):\n","        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n","\n","    lst_seq = []\n","    for i in range(0, len(text_data)):\n","        # token = tknzr.tokenize(text_data[i])\n","        token = custom_tokenizer(text_data[i])\n","        if len(token) > MAX_LEN:\n","            MAX_LEN = len(token)\n","            \n","        seq = np.zeros(len(token), dtype=int)\n","        for j in range(0, len(token)):\n","            for t in token_labels[i]:\n","                # if token[j] in tknzr.tokenize(t):\n","                if token[j] in custom_tokenizer(t):\n","                    seq[j] = 1\n","        lst_seq.append(seq)     \n","\n","    return (token_labels, lst_seq)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jI5OfwZYPaMO"},"source":["from copy import deepcopy\n","\n","# convert data\n","data['token'], data['seq'] = span_convert(text_data, spans)\n","dev['token'], dev['seq'] = span_convert(text_data_dev, spans_dev)\n","test['token'], test['seq'] = span_convert(text_data_test, spans_test)\n","\n","train = deepcopy(data)\n","data = pd.concat([data, dev])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WaHOdIeNKBqd"},"source":["# Evaluation metric "]},{"cell_type":"code","metadata":{"id":"as0mrBcck7eR"},"source":["# Evaluation metric\n","\n","import sys\n","import os\n","import os.path\n","from scipy.stats import sem\n","import numpy as np\n","from ast import literal_eval\n","\n","def f1(predictions, gold):\n","    \"\"\"\n","    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n","    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n","    :param predictions: a list of predicted offsets\n","    :param gold: a list of offsets serving as the ground truth\n","    :return: a score between 0 and 1\n","    \"\"\"\n","    if len(gold) == 0:\n","        return 1. if len(predictions) == 0 else 0.\n","    if len(predictions) == 0:\n","        return 0.\n","    predictions_set = set(predictions)\n","    gold_set = set(gold)\n","    nom = 2 * len(predictions_set.intersection(gold_set))\n","    denom = len(predictions_set) + len(gold_set)\n","    return float(nom)/float(denom)\n","\n","\n","def evaluate(pred, gold):\n","    \"\"\"\n","    Based on https://github.com/felipebravom/EmoInt/blob/master/codalab/scoring_program/evaluation.py\n","    :param pred: file with predictions\n","    :param gold: file with ground truth\n","    :return:\n","    \"\"\"\n","    # # read the predictions\n","    # pred_lines = pred.readlines()\n","    # # read the ground truth\n","    # gold_lines = gold.readlines()\n","\n","    pred_lines = pred\n","    gold_lines = gold\n","\n","    # only when the same number of lines exists\n","    if (len(pred_lines) == len(gold_lines)):\n","        data_dic = {}\n","        for n, line in enumerate(gold_lines):\n","            parts = line.split('\\t')\n","            if len(parts) == 2:\n","                data_dic[int(parts[0])] = [literal_eval(parts[1])]\n","            else:\n","                raise ValueError('Format problem for gold line %d.', n)\n","\n","        for n, line in enumerate(pred_lines):\n","            parts = line.split('\\t')\n","            if len(parts) == 2:\n","                if int(parts[0]) in data_dic:\n","                    try:\n","                        data_dic[int(parts[0])].append(literal_eval(parts[1]))\n","                    except ValueError:\n","                        # Invalid predictions are replaced by a default value\n","                        data_dic[int(parts[0])].append([])\n","                else:\n","                    raise ValueError('Invalid text id for pred line %d.', n)\n","            else:\n","                raise ValueError('Format problem for pred line %d.', n)\n","\n","        # lists storing gold and prediction scores\n","        scores = []\n","        for id in data_dic:\n","            if len(data_dic[id]) == 2:\n","                gold_spans = data_dic[id][0]\n","                pred_spans = data_dic[id][1]\n","                scores.append(f1(pred_spans, gold_spans))\n","            else:\n","                sys.exit('Repeated id in test data.')\n","\n","        return (np.mean(scores), sem(scores))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G2WZCOctatQs"},"source":["# Data analysis "]},{"cell_type":"code","metadata":{"id":"05MJABVLa97Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613034436786,"user_tz":-420,"elapsed":5537,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"00eeac09-fc55-445d-873e-944768e60fe7"},"source":["# Number of training data\n","print(len(train))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["7939\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3jc4SouVmH4","executionInfo":{"status":"ok","timestamp":1613034436789,"user_tz":-420,"elapsed":4666,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"89f2a98d-7759-4e67-a559-852da86467cf"},"source":["# Number of test data\n","print(len(test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"id":"M9gTn4FLV4rL","executionInfo":{"status":"ok","timestamp":1613034436789,"user_tz":-420,"elapsed":2930,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"66317e96-77ee-4ea5-91b6-9f4e522ec65e"},"source":["test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>spans</th>\n","      <th>text</th>\n","      <th>token</th>\n","      <th>seq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...</td>\n","      <td>That's right. They are not normal. And I am st...</td>\n","      <td>[ABNORMAL, sexist rubbish]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[81, 82, 83, 84, 85, 86]</td>\n","      <td>\"Watch people die from taking away their healt...</td>\n","      <td>[stupid]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[]</td>\n","      <td>tens years ago i contacted the PDR and suggest...</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[]</td>\n","      <td>The parallels between the ANC and the Sicilian...</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[]</td>\n","      <td>Intel Community: ‘How can we work for a Presid...</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>[4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...</td>\n","      <td>hey loser change your name to something more a...</td>\n","      <td>[loser, ignorant, loser]</td>\n","      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>[23, 24, 25, 26, 27]</td>\n","      <td>And you are a complete moron who obviously doe...</td>\n","      <td>[moron]</td>\n","      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>[157, 158, 159, 160, 161, 162, 163, 164, 165, ...</td>\n","      <td>Such vitriol from the left.  Who would have th...</td>\n","      <td>[hypocrites]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>[]</td>\n","      <td>It is now time for most of you to expand your ...</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>[828, 829, 830, 831]</td>\n","      <td>Why does this author think she can demand, or ...</td>\n","      <td>[dumb]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2000 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                  spans  ...                                                seq\n","0     [84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1                              [81, 82, 83, 84, 85, 86]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","2                                                    []  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","3                                                    []  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","4                                                    []  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","...                                                 ...  ...                                                ...\n","1995  [4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...  ...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n","1996                               [23, 24, 25, 26, 27]  ...  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1997  [157, 158, 159, 160, 161, 162, 163, 164, 165, ...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1998                                                 []  ...         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","1999                               [828, 829, 830, 831]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","\n","[2000 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"tu3EjmPXbB8A","executionInfo":{"status":"ok","timestamp":1613034447824,"user_tz":-420,"elapsed":1043,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"5d4890e3-fb1f-4c29-8dc4-08f32dc16cc3"},"source":["# Show example of training data\n","train.tail(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>spans</th>\n","      <th>text</th>\n","      <th>token</th>\n","      <th>seq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7919</th>\n","      <td>[49, 50, 51, 52, 53, 54]</td>\n","      <td>Ah, so sad. It certainly does seem to bother t...</td>\n","      <td>[trolls]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7920</th>\n","      <td>[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...</td>\n","      <td>Yes, let's end the insanity!  No more trans pe...</td>\n","      <td>[No more trans people! No more boys kissing bo...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7921</th>\n","      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n","      <td>Such garbage logic by republicans which will b...</td>\n","      <td>[Such garbage logic by republicans]</td>\n","      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7922</th>\n","      <td>[33, 34, 35, 36, 37, 38, 39, 40, 41, 96, 97, 9...</td>\n","      <td>aa the usual left wing attack of stupidity is ...</td>\n","      <td>[stupidity, crying that wont stop]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7923</th>\n","      <td>[37, 38, 39, 40, 41]</td>\n","      <td>The G&amp;M doesn't need to repeat EVERY idiot thi...</td>\n","      <td>[idiot]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7924</th>\n","      <td>[177, 178, 179, 180, 181, 182, 183, 184, 185]</td>\n","      <td>All this reality tv suspense, all this loud no...</td>\n","      <td>[stupidity]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7925</th>\n","      <td>[15, 16, 17, 18, 19, 20]</td>\n","      <td>Can't get over idiots like yourself who probab...</td>\n","      <td>[idiots]</td>\n","      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7926</th>\n","      <td>[31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 46, 4...</td>\n","      <td>I only use the word haole when stupidity and a...</td>\n","      <td>[stupidity, arrogance]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7927</th>\n","      <td>[132, 133, 134, 135]</td>\n","      <td>Pity Vegter doesn't apply the same rigour he n...</td>\n","      <td>[dick]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7928</th>\n","      <td>[54, 55, 56, 57]</td>\n","      <td>Anyone who voted for Hillary is blind, gullibl...</td>\n","      <td>[fool]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7929</th>\n","      <td>[26, 27, 28, 29, 30, 114, 115, 116, 117, 118, ...</td>\n","      <td>No accountability on both sides. No clue on wh...</td>\n","      <td>[sides, idiots]</td>\n","      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7930</th>\n","      <td>[320, 321, 322, 323, 324, 350, 351, 352, 389, ...</td>\n","      <td>Yes, it is OK. I haven't seen any rioting and ...</td>\n","      <td>[dirty, gay, racist, morons]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7931</th>\n","      <td>[44, 45, 46, 47, 48, 49, 50, 51]</td>\n","      <td>In other word, blah, blah, blah, blah. It's bu...</td>\n","      <td>[bullshit]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7932</th>\n","      <td>[218, 219, 220, 221, 222, 223]</td>\n","      <td>Driving like that is why people are going to s...</td>\n","      <td>[idiots]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7933</th>\n","      <td>[4, 5, 6, 7, 8, 35, 36, 37, 38]</td>\n","      <td>The idiot in chief can't speak for sh!t 😜👎</td>\n","      <td>[idiot, sh!t]</td>\n","      <td>[0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7934</th>\n","      <td>[8, 9, 10, 11]</td>\n","      <td>Another fool pipes in.</td>\n","      <td>[fool]</td>\n","      <td>[0, 1, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7935</th>\n","      <td>[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...</td>\n","      <td>So if a restaurant owner puts up a sign saying...</td>\n","      <td>[No Blacks Allowed]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7936</th>\n","      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n","      <td>Any faith that can't stand up to logic and rea...</td>\n","      <td>[Any faith that can't stand up to logic and re...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7937</th>\n","      <td>[5, 6, 7, 8, 9, 10, 11]</td>\n","      <td>This idiotic. Use the surplus to pay down the ...</td>\n","      <td>[idiotic]</td>\n","      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7938</th>\n","      <td>[106, 107, 108, 109, 110, 169, 170, 171, 172, ...</td>\n","      <td>Who is this \"we\" of which you speak? Are you r...</td>\n","      <td>[penis, women's bodies]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  spans  ...                                                seq\n","7919                           [49, 50, 51, 52, 53, 54]  ...         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n","7920  [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...  ...  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","7921  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  ...   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","7922  [33, 34, 35, 36, 37, 38, 39, 40, 41, 96, 97, 9...  ...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n","7923                               [37, 38, 39, 40, 41]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n","7924      [177, 178, 179, 180, 181, 182, 183, 184, 185]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7925                           [15, 16, 17, 18, 19, 20]  ...  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7926  [31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 46, 4...  ...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ...\n","7927                               [132, 133, 134, 135]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7928                                   [54, 55, 56, 57]  ...            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n","7929  [26, 27, 28, 29, 30, 114, 115, 116, 117, 118, ...  ...  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7930  [320, 321, 322, 323, 324, 350, 351, 352, 389, ...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7931                   [44, 45, 46, 47, 48, 49, 50, 51]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n","7932                     [218, 219, 220, 221, 222, 223]  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7933                    [4, 5, 6, 7, 8, 35, 36, 37, 38]  ...               [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n","7934                                     [8, 9, 10, 11]  ...                                    [0, 1, 0, 0, 0]\n","7935  [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, ...\n","7936  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  ...         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n","7937                            [5, 6, 7, 8, 9, 10, 11]  ...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","7938  [106, 107, 108, 109, 110, 169, 170, 171, 172, ...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","\n","[20 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"viHoxDqEauyj"},"source":["# counting word in spans for train \n","len_span = train['token'].apply(len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sw7qNdR-VppI"},"source":["# counting word in spans for test \n","len_span_test = test['token'].apply(len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrGstC0whNKs","executionInfo":{"status":"ok","timestamp":1613034465286,"user_tz":-420,"elapsed":1077,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"84f32782-36eb-4a21-ef8a-a26a080f8d55"},"source":["# Statistic spans by number of word in span for train \n","len_span.value_counts(normalize=True, sort=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1     0.676534\n","2     0.200655\n","0     0.061091\n","3     0.043330\n","4     0.011588\n","5     0.003401\n","6     0.001512\n","7     0.001008\n","8     0.000504\n","11    0.000126\n","25    0.000126\n","9     0.000126\n","Name: token, dtype: float64"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUR6XwF2VuEz","executionInfo":{"status":"ok","timestamp":1613034459712,"user_tz":-420,"elapsed":1008,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"4a01a550-5700-4d4e-bca8-b91d7d65dea5"},"source":["# Statistic spans by number of word in span for test \n","len_span_test.value_counts(normalize=True, sort=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    0.7035\n","0    0.1970\n","2    0.0860\n","3    0.0080\n","4    0.0040\n","6    0.0010\n","7    0.0005\n","Name: token, dtype: float64"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"gPJWuIpegKo1","executionInfo":{"status":"ok","timestamp":1613034461050,"user_tz":-420,"elapsed":1446,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"6d3b9b0f-1523-42bc-eabc-09d6bf1d4c62"},"source":["# Distribution histogram plot\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","%matplotlib inline\n","\n","fig, ax = plt.subplots()\n","\n","ax.hist(len_span, density=True, edgecolor='k', rwidth=0.8)  # density=False would make counts\n","\n","plt.ylabel('Percentage')\n","plt.xlabel('Number of span');"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"dq3YWgcPFkB1"},"source":["# Word embedding"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s4txBO5zya-5","executionInfo":{"status":"ok","timestamp":1613030249939,"user_tz":-420,"elapsed":19480,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"a3703690-324f-4d99-c80a-0b4171c205b0"},"source":["# Read embedding\n","word_dict = []\n","embeddings_index = {}\n","f = open('drive/My Drive/CODE/SemVal/embedding/glove.twitter.27B.25d.txt')\n","for line in f:\n","    values = line.split(' ')\n","    word = values[0] \n","    word_dict.append(word)\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('GloVe data loaded')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GloVe data loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n8bsiH8BSGjC"},"source":["words = word_dict\n","num_words = len(words)\n","\n","# Dictionary word:index pair\n","# word is key and its value is corresponding index\n","word_to_index = {w : i + 2 for i, w in enumerate(words)}\n","word_to_index[\"UNK\"] = 1\n","word_to_index[\"PAD\"] = 0\n","\n","# Dictionary lable:index pair\n","idx2word = {i: w for w, i in word_to_index.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OBCNzNnas7p"},"source":["# first create a matrix of zeros, this is our embedding matrix\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","\n","# for each word in out tokenizer lets try to find that work in our w2v model\n","for word, i in word_to_index.items():\n","    if i > max_feature:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # we found the word - add that words vector to the matrix\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # doesn't exist, assign a random vector\n","        embedding_matrix[i] = np.random.randn(embedding_dim)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"exfoi9XHba3G"},"source":[" # mapping for token cases\n","case2Idx = {'1': 1, '0': 0}\n","caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n","\n","char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n","for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n","    char2Idx[c] = len(char2Idx)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BmJ-NOODFb0Y"},"source":["# Data pre-processing "]},{"cell_type":"code","metadata":{"id":"ZpsRfA9QF9mL"},"source":["from sklearn.model_selection import train_test_split\n","\n","y = data['seq']\n","X = data['text']\n","\n","y_test = test['seq']\n","X_test = test['text']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bsS9aVEVcJQO"},"source":["#train test\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size = 0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z8GGEAurVu_c","executionInfo":{"status":"ok","timestamp":1613030250911,"user_tz":-420,"elapsed":19143,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"17065691-6cc6-496a-d90e-d8b860869a20"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","from nltk.tokenize import TweetTokenizer\n","from nltk.tokenize import word_tokenize\n","from nltk import WordNetLemmatizer\n","\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.initializers import Constant\n","from nltk.corpus import stopwords\n","import re\n","import numpy as np\n","\n","tknzr2 = TweetTokenizer()\n","\n","def custom_tokenizer(text_data):\n","    text_data = text_data.lower()\n","    return tknzr2.tokenize(text_data)\n","\n","def preprocess(text):\n","    text = text.lower()\n","\n","    word_list = nltk.word_tokenize(text)\n","    lemma = WordNetLemmatizer()\n","\n","    for w in word_list:\n","        w = lemma.lemmatize(w)\n","\n","    new_text = \"\"\n","    for w in word_list:\n","        new_text = new_text + \" \" + w\n","\n","    return new_text\n","\n","def encoding(X, y, isTest = True):\n","    sentences = []\n","    \n","    for t in X:\n","        sentences.append(custom_tokenizer(t))\n","\n","    X = []\n","    for s in sentences:\n","        sent = []\n","        for w in s:\n","            try:\n","                w = w.lower()\n","                sent.append(word_to_index[w])\n","            except:\n","                sent.append(word_to_index[\"UNK\"])\n","        X.append(sent)\n","           \n","    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])\n","\n","    if isTest:\n","        y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=word_to_index[\"PAD\"])\n","        y = to_categorical(y, num_classes=2)\n","    else:\n","        y = None\n","\n","    return (X,y)\n","\n","\n","def decoding(text_data, encoding_text, prediction):\n","    test = [[idx2word[i] for i in row] for row in encoding_text]\n","\n","    lst_token = []\n","\n","    for t in range(0, len(test)):\n","        yy_pred = []\n","        for i in range(0, len(test[t])):\n","            if prediction[t][i] == 1:\n","                yy_pred.append(test[t][i])\n","        lst_token.append(yy_pred)\n","\n","    lis_idx = []\n","    for i in range(0, len(text_data)):\n","        idx = []\n","        for t in lst_token[i]:\n","            index = text_data[i].find(t)\n","            idx.append(index)\n","            for j in range(1, len(t)):\n","                index = index + 1\n","                idx.append(index)\n","        lis_idx.append(idx)\n","\n","    return lis_idx"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"9ajE_t3U-PiF"},"source":["X1, y1 = encoding(X_train, y_train)\n","X2, y2 = encoding(X_dev, y_dev)\n","X3, y3 = encoding(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozUGBMYyIAAu","executionInfo":{"status":"ok","timestamp":1613030253088,"user_tz":-420,"elapsed":21310,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64","userId":"09824077883060402796"}},"outputId":"7912ee18-23c8-469a-c207-492183eb67c8"},"source":["# Illustrating the data transforming \n","x_t, y_t = encoding(X, y)\n","\n","print(custom_tokenizer(X[7926]))\n","print(X[7926])\n","print(x_t[7926])\n","\n","print(y[7926])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['i', 'only', 'use', 'the', 'word', 'haole', 'when', 'stupidity', 'and', 'arrogance', 'is', 'involved', 'and', 'not', 'all', 'the', 'time', '.', 'excluding', 'the', 'potus', 'of', 'course', '.']\n","I only use the word haole when stupidity and arrogance is involved and not all the time.  Excluding the POTUS of course.\n","[    12    216    718     15    894 724236     94  17046     28  48680\n","     34   6187     28     80     77     15    137      3  81507     15\n","  34058     41   1605      3      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0]\n","[0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"onI2IhquAwPi"},"source":["# Classification "]},{"cell_type":"code","metadata":{"id":"N0wNXoS4Ax1J"},"source":["import numpy as np \n","import pandas as pd \n","from transformers import DistilBertTokenizerFast\n","tokenizer = DistilBertTokenizerFast.from_pretrained('unitary/toxic-bert')\n","\n","import torch\n","\n","class BuildDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","X_train_classify = text_data\n","y_train_classify = lbl\n","\n","X_test_classify = text_data_test\n","y_test_classify = lbl_test\n","\n","\n","train_encodings = tokenizer(X_train_classify.tolist(), truncation=True, padding=True, max_length=max_len)\n","test_encodings = tokenizer(X_test_classify.tolist(), truncation=True, padding=True, max_length=max_len)\n","\n","# train = pd.DataFrame({'text': train_encodings, 'labels': y_train_classify})\n","# # dev = pd.DataFrame({'text': dev_X, 'labels': dev_y})\n","# test = pd.DataFrame({'text': test_encodings, 'labels': y_test_classify})\n","\n","# train = pd.concat([train, test])\n","\n","train_dataset = BuildDataset(train_encodings, y_train_classify)\n","test_dataset = BuildDataset(test_encodings, y_test_classify)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hdUD-pNU6ua"},"source":["from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, AutoModelForSequenceClassification\n","\n","\n","training_args = TrainingArguments(\n","    output_dir='drive/MyDrive/CODE/SemVal/results',          # output directory\n","    num_train_epochs=3,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n",")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=test_dataset             # evaluation dataset\n",")\n","\n","trainer.train()\n","\n","trainer.save_model('drive/MyDrive/CODE/SemVal/model/transformers2/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yG2B766GfFXX"},"source":["y_pred_classify = trainer.predict(test_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRpJg0QYkCbf"},"source":["from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score\n","\n","y_pred = y_pred_classify.label_ids\n","y_true = y_test_classify\n","\n","cf = confusion_matrix(y_true, y_pred)\n","print(cf)\n","\n","evaluation = f1_score(y_true, y_pred, average='micro')\n","\n","print(\"F1 - micro: \" + str(evaluation))\n","\n","evaluation = f1_score(y_true, y_pred, average='macro')\n","print(\"F1 - macro: \" + str(evaluation))\n","\n","evaluation = recall_score(y_true, y_pred)\n","print(\"Recall: \" + str(evaluation))"],"execution_count":null,"outputs":[]}]}